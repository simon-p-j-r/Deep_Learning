{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n",
      "sys.version_info(major=3, minor=6, micro=9, releaselevel='final', serial=0)\n",
      "matplotlib 3.2.1\n",
      "numpy 1.18.5\n",
      "pandas 1.0.4\n",
      "sklearn 0.23.1\n",
      "tensorflow 2.2.0\n",
      "tensorflow.keras 2.3.0-tf\n"
     ]
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "print(tf.__version__)\n",
    "print(sys.version_info)\n",
    "for module in mpl, np, pd, sklearn, tf, keras:\n",
    "    print(module.__name__, module.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.keras.layers.core.Dense'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10, 100), dtype=float32, numpy=\n",
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# layer = tf.keras.layers.Dense(100)\n",
    "layer = tf.keras.layers.Dense(100, input_shape=(None, 5)) #input_shape我们往往第一层指定\n",
    "# layer是输入为5，输出为100的全连接层，所以对于输入为10x5的矩阵来看，会乘以一个5x100的矩阵，所以输出就是10x100.\n",
    "print(type(layer))\n",
    "layer(tf.zeros([10, 5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'dense/kernel:0' shape=(5, 100) dtype=float32, numpy=\n",
      "array([[-0.06453846, -0.12320153, -0.13491979,  0.17714934,  0.11429717,\n",
      "         0.11485256,  0.1643021 , -0.03201869,  0.16556768, -0.01204546,\n",
      "        -0.06900437,  0.1035618 ,  0.22894795, -0.13811088,  0.07596828,\n",
      "        -0.12119475,  0.20349471,  0.15923156,  0.19122888,  0.08080117,\n",
      "        -0.01723489, -0.00962064,  0.21747093, -0.10737619, -0.16744527,\n",
      "        -0.22790873, -0.14590228,  0.17100956, -0.08213116,  0.09444307,\n",
      "         0.1957054 ,  0.14308067,  0.08154817,  0.10154136,  0.20162068,\n",
      "         0.09937923, -0.02451335, -0.1940694 , -0.13998345, -0.22956067,\n",
      "         0.12432902, -0.0246544 , -0.20672406,  0.08075307, -0.16801605,\n",
      "         0.12738328,  0.06394608,  0.02090324, -0.09075761, -0.21058105,\n",
      "        -0.17678316, -0.13567728,  0.06494544, -0.01262508, -0.04382712,\n",
      "         0.10391884, -0.23311812,  0.0639206 , -0.11974918, -0.21400222,\n",
      "         0.1303191 , -0.02567486,  0.21754234, -0.18932383, -0.02277517,\n",
      "        -0.17284843, -0.22166169,  0.20849757,  0.0282584 , -0.18450011,\n",
      "        -0.2030854 ,  0.17467807,  0.07236455, -0.22512326,  0.09308298,\n",
      "        -0.08356123,  0.11043607, -0.2217266 ,  0.16358028, -0.01804812,\n",
      "         0.05009048,  0.07059567,  0.14321302, -0.07129766,  0.18281125,\n",
      "         0.18414705,  0.0530843 ,  0.09117724, -0.06171919,  0.19893728,\n",
      "         0.11364667,  0.1717142 ,  0.18495055,  0.10363494,  0.05288632,\n",
      "         0.10282086,  0.17645688, -0.17308198,  0.06145434, -0.1648514 ],\n",
      "       [-0.13528085,  0.05349402, -0.15685335,  0.16599844,  0.15602474,\n",
      "        -0.12060476, -0.05349244,  0.01284142,  0.01768617, -0.01674858,\n",
      "        -0.13529041,  0.18529369,  0.17228164,  0.0373982 , -0.05586101,\n",
      "         0.07857509,  0.11651473, -0.18280406,  0.21976973, -0.0799851 ,\n",
      "         0.13664354, -0.07487442,  0.0881087 ,  0.21358179,  0.1578343 ,\n",
      "         0.02293111, -0.13690212, -0.05956349, -0.07595643,  0.02096041,\n",
      "         0.06483693, -0.09021105,  0.01727982, -0.07836615,  0.12672244,\n",
      "        -0.06827287,  0.08586313, -0.04913977,  0.11677505, -0.1537742 ,\n",
      "        -0.09383883,  0.08732839,  0.04072328, -0.03203283,  0.10285152,\n",
      "         0.14594467,  0.21491532, -0.16019216,  0.09848516, -0.12536287,\n",
      "        -0.14205691,  0.13269182,  0.15209456, -0.11442729,  0.19866692,\n",
      "         0.00113797, -0.01326504,  0.04197319,  0.07941978, -0.03849714,\n",
      "         0.06760909, -0.2143871 , -0.12515873, -0.17567037, -0.17996667,\n",
      "        -0.15445007, -0.00097184, -0.18460971,  0.05909102, -0.1891561 ,\n",
      "        -0.12505546,  0.11776941, -0.06510122, -0.18700023, -0.13971671,\n",
      "        -0.19050477,  0.08487509,  0.08569916,  0.1275913 , -0.21465184,\n",
      "        -0.0737565 ,  0.00059296,  0.13541083,  0.07100062,  0.05097876,\n",
      "         0.02327676, -0.1191181 , -0.03621389,  0.10780562,  0.07470568,\n",
      "        -0.02023472,  0.05641024, -0.11858653, -0.0292469 , -0.16303116,\n",
      "        -0.09895349, -0.02100709,  0.09311758,  0.04382689, -0.21231838],\n",
      "       [ 0.17006932, -0.0163099 , -0.19657317, -0.08534563, -0.01654944,\n",
      "        -0.19160903,  0.21479763,  0.18634821, -0.22859977, -0.04799923,\n",
      "         0.04164524, -0.08119534, -0.06116192,  0.1921724 , -0.01189157,\n",
      "        -0.1836891 ,  0.1749986 , -0.1533191 ,  0.16131221,  0.13284324,\n",
      "        -0.01524869,  0.0225945 , -0.17583194,  0.17739497, -0.01839413,\n",
      "        -0.08885889, -0.08230595, -0.20650652, -0.0694332 , -0.16541956,\n",
      "        -0.16518481, -0.16831008, -0.15157034, -0.10755907,  0.09207739,\n",
      "        -0.12537661,  0.23469202,  0.14884882, -0.09635973,  0.0314825 ,\n",
      "        -0.04739647, -0.01668012,  0.08898737, -0.10435237,  0.15953572,\n",
      "        -0.14899835,  0.15041481,  0.07944269, -0.01050813, -0.18003473,\n",
      "         0.15570392, -0.21819629,  0.05013715, -0.08794495, -0.07011785,\n",
      "        -0.22530678,  0.14986818, -0.22607897, -0.02928765,  0.18026571,\n",
      "         0.04871197, -0.18633261, -0.0477643 ,  0.17522411,  0.13189472,\n",
      "         0.02358995, -0.04013933, -0.18286699, -0.16407874,  0.1148047 ,\n",
      "         0.09800039, -0.21741849,  0.19840024,  0.20160656,  0.10640188,\n",
      "        -0.10494401, -0.22551554,  0.05028944,  0.10291819, -0.00901087,\n",
      "        -0.05977722,  0.12455191,  0.18452267,  0.1809391 ,  0.16632952,\n",
      "         0.15938692, -0.01660523,  0.04482625,  0.21695293, -0.23146892,\n",
      "        -0.06799577, -0.09108971,  0.13422449, -0.21504657, -0.21412072,\n",
      "        -0.23328897,  0.21703978,  0.14563577,  0.04228579, -0.21857238],\n",
      "       [-0.23178431,  0.10738252, -0.23566319, -0.18295555,  0.1574576 ,\n",
      "         0.07445808,  0.21503957,  0.2289481 ,  0.01115744, -0.12120882,\n",
      "         0.12108104, -0.06757322,  0.02725168, -0.20956664,  0.0417801 ,\n",
      "         0.08790614, -0.00693667, -0.23487441, -0.00622232,  0.12383746,\n",
      "        -0.20609918,  0.17899986, -0.0457129 ,  0.17178424,  0.17149113,\n",
      "        -0.21803157, -0.03944311,  0.16654016,  0.05397688, -0.0071684 ,\n",
      "         0.01992883, -0.16997074, -0.15151191, -0.18725322, -0.03403676,\n",
      "         0.17610489, -0.01160638, -0.12889774, -0.21101671, -0.11319061,\n",
      "         0.21617599, -0.02326088,  0.05093841,  0.1757481 , -0.14333081,\n",
      "         0.19905747,  0.18886776,  0.18050008, -0.06800774,  0.14897002,\n",
      "         0.01150061,  0.13102774, -0.0662356 , -0.21637753,  0.00193942,\n",
      "         0.07160045, -0.15198645,  0.22497918,  0.05235647,  0.06373902,\n",
      "         0.1169662 ,  0.17274685, -0.21172348, -0.04785429,  0.1976649 ,\n",
      "        -0.22912543,  0.11120321,  0.21435551, -0.00188498, -0.14128402,\n",
      "         0.11887793,  0.07028495, -0.20677176,  0.12487616,  0.03342004,\n",
      "        -0.19182593, -0.06362475,  0.10602199, -0.08657974,  0.04676493,\n",
      "         0.07353316,  0.0846373 , -0.05429694, -0.21789679, -0.02451278,\n",
      "         0.16051875, -0.16211495,  0.0616902 , -0.11992244, -0.07339671,\n",
      "         0.11751352,  0.2117563 , -0.21060584, -0.16051841,  0.17093755,\n",
      "        -0.20008393,  0.07894777,  0.06971626,  0.08256717,  0.00166653],\n",
      "       [-0.00472312,  0.09821223, -0.15742329, -0.18459587, -0.0974019 ,\n",
      "        -0.1321553 ,  0.0849994 ,  0.1802371 , -0.18556434, -0.00746949,\n",
      "         0.06052621, -0.18792546, -0.07398891,  0.02342878,  0.16806956,\n",
      "         0.03867348,  0.10917129,  0.03355499, -0.01270567,  0.06117423,\n",
      "         0.14217944, -0.00585961,  0.03684564,  0.0293002 , -0.20634933,\n",
      "        -0.20293328,  0.16370066,  0.23219658, -0.13523638,  0.17544256,\n",
      "         0.11028789, -0.1649391 ,  0.13379173,  0.10050009, -0.12820978,\n",
      "         0.20794018, -0.05010791,  0.22212584, -0.21526992, -0.19037074,\n",
      "        -0.07409526, -0.02053627,  0.21326251,  0.15511768,  0.20590957,\n",
      "         0.23000367,  0.12855224,  0.01082028,  0.00536777,  0.09603618,\n",
      "        -0.06047066,  0.12006436,  0.12503357, -0.07776093,  0.2240894 ,\n",
      "         0.18980001,  0.17035173,  0.13557328,  0.08045818,  0.2074547 ,\n",
      "        -0.03244004, -0.09670585,  0.16510741,  0.1413111 , -0.2375288 ,\n",
      "         0.10066463, -0.01114777,  0.04027371,  0.05057873,  0.14390011,\n",
      "         0.14948563, -0.10148641, -0.02797207,  0.2171932 , -0.19353475,\n",
      "         0.00810646, -0.23027804,  0.11758156, -0.01494081,  0.16686685,\n",
      "        -0.19661225, -0.20135064,  0.13761242, -0.13309819, -0.08059582,\n",
      "         0.06290574, -0.19932826, -0.06087844,  0.22836457,  0.2327839 ,\n",
      "         0.15470894, -0.08514078,  0.09744401, -0.08314785, -0.03330851,\n",
      "         0.1098891 ,  0.21219902, -0.1291953 ,  0.159216  ,  0.02678452]],\n",
      "      dtype=float32)>, <tf.Variable 'dense/bias:0' shape=(100,) dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32)>]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'dense/kernel:0' shape=(5, 100) dtype=float32, numpy=\n",
       " array([[-0.06453846, -0.12320153, -0.13491979,  0.17714934,  0.11429717,\n",
       "          0.11485256,  0.1643021 , -0.03201869,  0.16556768, -0.01204546,\n",
       "         -0.06900437,  0.1035618 ,  0.22894795, -0.13811088,  0.07596828,\n",
       "         -0.12119475,  0.20349471,  0.15923156,  0.19122888,  0.08080117,\n",
       "         -0.01723489, -0.00962064,  0.21747093, -0.10737619, -0.16744527,\n",
       "         -0.22790873, -0.14590228,  0.17100956, -0.08213116,  0.09444307,\n",
       "          0.1957054 ,  0.14308067,  0.08154817,  0.10154136,  0.20162068,\n",
       "          0.09937923, -0.02451335, -0.1940694 , -0.13998345, -0.22956067,\n",
       "          0.12432902, -0.0246544 , -0.20672406,  0.08075307, -0.16801605,\n",
       "          0.12738328,  0.06394608,  0.02090324, -0.09075761, -0.21058105,\n",
       "         -0.17678316, -0.13567728,  0.06494544, -0.01262508, -0.04382712,\n",
       "          0.10391884, -0.23311812,  0.0639206 , -0.11974918, -0.21400222,\n",
       "          0.1303191 , -0.02567486,  0.21754234, -0.18932383, -0.02277517,\n",
       "         -0.17284843, -0.22166169,  0.20849757,  0.0282584 , -0.18450011,\n",
       "         -0.2030854 ,  0.17467807,  0.07236455, -0.22512326,  0.09308298,\n",
       "         -0.08356123,  0.11043607, -0.2217266 ,  0.16358028, -0.01804812,\n",
       "          0.05009048,  0.07059567,  0.14321302, -0.07129766,  0.18281125,\n",
       "          0.18414705,  0.0530843 ,  0.09117724, -0.06171919,  0.19893728,\n",
       "          0.11364667,  0.1717142 ,  0.18495055,  0.10363494,  0.05288632,\n",
       "          0.10282086,  0.17645688, -0.17308198,  0.06145434, -0.1648514 ],\n",
       "        [-0.13528085,  0.05349402, -0.15685335,  0.16599844,  0.15602474,\n",
       "         -0.12060476, -0.05349244,  0.01284142,  0.01768617, -0.01674858,\n",
       "         -0.13529041,  0.18529369,  0.17228164,  0.0373982 , -0.05586101,\n",
       "          0.07857509,  0.11651473, -0.18280406,  0.21976973, -0.0799851 ,\n",
       "          0.13664354, -0.07487442,  0.0881087 ,  0.21358179,  0.1578343 ,\n",
       "          0.02293111, -0.13690212, -0.05956349, -0.07595643,  0.02096041,\n",
       "          0.06483693, -0.09021105,  0.01727982, -0.07836615,  0.12672244,\n",
       "         -0.06827287,  0.08586313, -0.04913977,  0.11677505, -0.1537742 ,\n",
       "         -0.09383883,  0.08732839,  0.04072328, -0.03203283,  0.10285152,\n",
       "          0.14594467,  0.21491532, -0.16019216,  0.09848516, -0.12536287,\n",
       "         -0.14205691,  0.13269182,  0.15209456, -0.11442729,  0.19866692,\n",
       "          0.00113797, -0.01326504,  0.04197319,  0.07941978, -0.03849714,\n",
       "          0.06760909, -0.2143871 , -0.12515873, -0.17567037, -0.17996667,\n",
       "         -0.15445007, -0.00097184, -0.18460971,  0.05909102, -0.1891561 ,\n",
       "         -0.12505546,  0.11776941, -0.06510122, -0.18700023, -0.13971671,\n",
       "         -0.19050477,  0.08487509,  0.08569916,  0.1275913 , -0.21465184,\n",
       "         -0.0737565 ,  0.00059296,  0.13541083,  0.07100062,  0.05097876,\n",
       "          0.02327676, -0.1191181 , -0.03621389,  0.10780562,  0.07470568,\n",
       "         -0.02023472,  0.05641024, -0.11858653, -0.0292469 , -0.16303116,\n",
       "         -0.09895349, -0.02100709,  0.09311758,  0.04382689, -0.21231838],\n",
       "        [ 0.17006932, -0.0163099 , -0.19657317, -0.08534563, -0.01654944,\n",
       "         -0.19160903,  0.21479763,  0.18634821, -0.22859977, -0.04799923,\n",
       "          0.04164524, -0.08119534, -0.06116192,  0.1921724 , -0.01189157,\n",
       "         -0.1836891 ,  0.1749986 , -0.1533191 ,  0.16131221,  0.13284324,\n",
       "         -0.01524869,  0.0225945 , -0.17583194,  0.17739497, -0.01839413,\n",
       "         -0.08885889, -0.08230595, -0.20650652, -0.0694332 , -0.16541956,\n",
       "         -0.16518481, -0.16831008, -0.15157034, -0.10755907,  0.09207739,\n",
       "         -0.12537661,  0.23469202,  0.14884882, -0.09635973,  0.0314825 ,\n",
       "         -0.04739647, -0.01668012,  0.08898737, -0.10435237,  0.15953572,\n",
       "         -0.14899835,  0.15041481,  0.07944269, -0.01050813, -0.18003473,\n",
       "          0.15570392, -0.21819629,  0.05013715, -0.08794495, -0.07011785,\n",
       "         -0.22530678,  0.14986818, -0.22607897, -0.02928765,  0.18026571,\n",
       "          0.04871197, -0.18633261, -0.0477643 ,  0.17522411,  0.13189472,\n",
       "          0.02358995, -0.04013933, -0.18286699, -0.16407874,  0.1148047 ,\n",
       "          0.09800039, -0.21741849,  0.19840024,  0.20160656,  0.10640188,\n",
       "         -0.10494401, -0.22551554,  0.05028944,  0.10291819, -0.00901087,\n",
       "         -0.05977722,  0.12455191,  0.18452267,  0.1809391 ,  0.16632952,\n",
       "          0.15938692, -0.01660523,  0.04482625,  0.21695293, -0.23146892,\n",
       "         -0.06799577, -0.09108971,  0.13422449, -0.21504657, -0.21412072,\n",
       "         -0.23328897,  0.21703978,  0.14563577,  0.04228579, -0.21857238],\n",
       "        [-0.23178431,  0.10738252, -0.23566319, -0.18295555,  0.1574576 ,\n",
       "          0.07445808,  0.21503957,  0.2289481 ,  0.01115744, -0.12120882,\n",
       "          0.12108104, -0.06757322,  0.02725168, -0.20956664,  0.0417801 ,\n",
       "          0.08790614, -0.00693667, -0.23487441, -0.00622232,  0.12383746,\n",
       "         -0.20609918,  0.17899986, -0.0457129 ,  0.17178424,  0.17149113,\n",
       "         -0.21803157, -0.03944311,  0.16654016,  0.05397688, -0.0071684 ,\n",
       "          0.01992883, -0.16997074, -0.15151191, -0.18725322, -0.03403676,\n",
       "          0.17610489, -0.01160638, -0.12889774, -0.21101671, -0.11319061,\n",
       "          0.21617599, -0.02326088,  0.05093841,  0.1757481 , -0.14333081,\n",
       "          0.19905747,  0.18886776,  0.18050008, -0.06800774,  0.14897002,\n",
       "          0.01150061,  0.13102774, -0.0662356 , -0.21637753,  0.00193942,\n",
       "          0.07160045, -0.15198645,  0.22497918,  0.05235647,  0.06373902,\n",
       "          0.1169662 ,  0.17274685, -0.21172348, -0.04785429,  0.1976649 ,\n",
       "         -0.22912543,  0.11120321,  0.21435551, -0.00188498, -0.14128402,\n",
       "          0.11887793,  0.07028495, -0.20677176,  0.12487616,  0.03342004,\n",
       "         -0.19182593, -0.06362475,  0.10602199, -0.08657974,  0.04676493,\n",
       "          0.07353316,  0.0846373 , -0.05429694, -0.21789679, -0.02451278,\n",
       "          0.16051875, -0.16211495,  0.0616902 , -0.11992244, -0.07339671,\n",
       "          0.11751352,  0.2117563 , -0.21060584, -0.16051841,  0.17093755,\n",
       "         -0.20008393,  0.07894777,  0.06971626,  0.08256717,  0.00166653],\n",
       "        [-0.00472312,  0.09821223, -0.15742329, -0.18459587, -0.0974019 ,\n",
       "         -0.1321553 ,  0.0849994 ,  0.1802371 , -0.18556434, -0.00746949,\n",
       "          0.06052621, -0.18792546, -0.07398891,  0.02342878,  0.16806956,\n",
       "          0.03867348,  0.10917129,  0.03355499, -0.01270567,  0.06117423,\n",
       "          0.14217944, -0.00585961,  0.03684564,  0.0293002 , -0.20634933,\n",
       "         -0.20293328,  0.16370066,  0.23219658, -0.13523638,  0.17544256,\n",
       "          0.11028789, -0.1649391 ,  0.13379173,  0.10050009, -0.12820978,\n",
       "          0.20794018, -0.05010791,  0.22212584, -0.21526992, -0.19037074,\n",
       "         -0.07409526, -0.02053627,  0.21326251,  0.15511768,  0.20590957,\n",
       "          0.23000367,  0.12855224,  0.01082028,  0.00536777,  0.09603618,\n",
       "         -0.06047066,  0.12006436,  0.12503357, -0.07776093,  0.2240894 ,\n",
       "          0.18980001,  0.17035173,  0.13557328,  0.08045818,  0.2074547 ,\n",
       "         -0.03244004, -0.09670585,  0.16510741,  0.1413111 , -0.2375288 ,\n",
       "          0.10066463, -0.01114777,  0.04027371,  0.05057873,  0.14390011,\n",
       "          0.14948563, -0.10148641, -0.02797207,  0.2171932 , -0.19353475,\n",
       "          0.00810646, -0.23027804,  0.11758156, -0.01494081,  0.16686685,\n",
       "         -0.19661225, -0.20135064,  0.13761242, -0.13309819, -0.08059582,\n",
       "          0.06290574, -0.19932826, -0.06087844,  0.22836457,  0.2327839 ,\n",
       "          0.15470894, -0.08514078,  0.09744401, -0.08314785, -0.03330851,\n",
       "          0.1098891 ,  0.21219902, -0.1291953 ,  0.159216  ,  0.02678452]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'dense/bias:0' shape=(100,) dtype=float32, numpy=\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       dtype=float32)>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#layer.variables 可以打印layer里包含的所有参数\n",
    "# x * w + b  w就是指层的参数，kernel就是w，b就是bias\n",
    "print(layer.variables)\n",
    "print('-'*50)\n",
    "#获得所有可训练的变量\n",
    "layer.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Dense in module tensorflow.python.keras.layers.core object:\n",
      "\n",
      "class Dense(tensorflow.python.keras.engine.base_layer.Layer)\n",
      " |  Just your regular densely-connected NN layer.\n",
      " |  \n",
      " |  `Dense` implements the operation:\n",
      " |  `output = activation(dot(input, kernel) + bias)`\n",
      " |  where `activation` is the element-wise activation function\n",
      " |  passed as the `activation` argument, `kernel` is a weights matrix\n",
      " |  created by the layer, and `bias` is a bias vector created by the layer\n",
      " |  (only applicable if `use_bias` is `True`).\n",
      " |  \n",
      " |  Note: If the input to the layer has a rank greater than 2, then `Dense`\n",
      " |  computes the dot product between the `inputs` and the `kernel` along the\n",
      " |  last axis of the `inputs` and axis 1 of the `kernel` (using `tf.tensordot`).\n",
      " |  For example, if input has dimensions `(batch_size, d0, d1)`,\n",
      " |  then we create a `kernel` with shape `(d1, units)`, and the `kernel` operates\n",
      " |  along axis 2 of the `input`, on every sub-tensor of shape `(1, 1, d1)`\n",
      " |  (there are `batch_size * d0` such sub-tensors).\n",
      " |  The output in this case will have shape `(batch_size, d0, units)`.\n",
      " |  \n",
      " |  Besides, layer attributes cannot be modified after the layer has been called\n",
      " |  once (except the `trainable` attribute).\n",
      " |  \n",
      " |  Example:\n",
      " |  \n",
      " |  ```python\n",
      " |  # as first layer in a sequential model:\n",
      " |  model = Sequential()\n",
      " |  model.add(Dense(32, input_shape=(16,)))\n",
      " |  # now the model will take as input arrays of shape (*, 16)\n",
      " |  # and output arrays of shape (*, 32)\n",
      " |  \n",
      " |  # after the first layer, you don't need to specify\n",
      " |  # the size of the input anymore:\n",
      " |  model.add(Dense(32))\n",
      " |  ```\n",
      " |  \n",
      " |  Arguments:\n",
      " |    units: Positive integer, dimensionality of the output space.\n",
      " |    activation: Activation function to use.\n",
      " |      If you don't specify anything, no activation is applied\n",
      " |      (ie. \"linear\" activation: `a(x) = x`).\n",
      " |    use_bias: Boolean, whether the layer uses a bias vector.\n",
      " |    kernel_initializer: Initializer for the `kernel` weights matrix.\n",
      " |    bias_initializer: Initializer for the bias vector.\n",
      " |    kernel_regularizer: Regularizer function applied to\n",
      " |      the `kernel` weights matrix.\n",
      " |    bias_regularizer: Regularizer function applied to the bias vector.\n",
      " |    activity_regularizer: Regularizer function applied to\n",
      " |      the output of the layer (its \"activation\")..\n",
      " |    kernel_constraint: Constraint function applied to\n",
      " |      the `kernel` weights matrix.\n",
      " |    bias_constraint: Constraint function applied to the bias vector.\n",
      " |  \n",
      " |  Input shape:\n",
      " |    N-D tensor with shape: `(batch_size, ..., input_dim)`.\n",
      " |    The most common situation would be\n",
      " |    a 2D input with shape `(batch_size, input_dim)`.\n",
      " |  \n",
      " |  Output shape:\n",
      " |    N-D tensor with shape: `(batch_size, ..., units)`.\n",
      " |    For instance, for a 2D input with shape `(batch_size, input_dim)`,\n",
      " |    the output would have shape `(batch_size, units)`.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Dense\n",
      " |      tensorflow.python.keras.engine.base_layer.Layer\n",
      " |      tensorflow.python.module.module.Module\n",
      " |      tensorflow.python.training.tracking.tracking.AutoTrackable\n",
      " |      tensorflow.python.training.tracking.base.Trackable\n",
      " |      tensorflow.python.keras.utils.version_utils.LayerVersionSelector\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, units, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, **kwargs)\n",
      " |  \n",
      " |  build(self, input_shape)\n",
      " |      Creates the variables of the layer (optional, for subclass implementers).\n",
      " |      \n",
      " |      This is a method that implementers of subclasses of `Layer` or `Model`\n",
      " |      can override if they need a state-creation step in-between\n",
      " |      layer instantiation and layer call.\n",
      " |      \n",
      " |      This is typically used to create the weights of `Layer` subclasses.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        input_shape: Instance of `TensorShape`, or list of instances of\n",
      " |          `TensorShape` if the layer expects a list of inputs\n",
      " |          (one instance per input).\n",
      " |  \n",
      " |  call(self, inputs)\n",
      " |      This is where the layer's logic lives.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          inputs: Input tensor, or list/tuple of input tensors.\n",
      " |          **kwargs: Additional keyword arguments.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor or list/tuple of tensors.\n",
      " |  \n",
      " |  compute_output_shape(self, input_shape)\n",
      " |      Computes the output shape of the layer.\n",
      " |      \n",
      " |      If the layer has not been built, this method will call `build` on the\n",
      " |      layer. This assumes that the layer will later be used with inputs that\n",
      " |      match the input shape provided here.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          input_shape: Shape tuple (tuple of integers)\n",
      " |              or list of shape tuples (one per output tensor of the layer).\n",
      " |              Shape tuples can include None for free dimensions,\n",
      " |              instead of an integer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An input shape tuple.\n",
      " |  \n",
      " |  get_config(self)\n",
      " |      Returns the config of the layer.\n",
      " |      \n",
      " |      A layer config is a Python dictionary (serializable)\n",
      " |      containing the configuration of a layer.\n",
      " |      The same layer can be reinstantiated later\n",
      " |      (without its trained weights) from this configuration.\n",
      " |      \n",
      " |      The config of a layer does not include connectivity\n",
      " |      information, nor the layer class name. These are handled\n",
      " |      by `Network` (one layer of abstraction above).\n",
      " |      \n",
      " |      Returns:\n",
      " |          Python dictionary.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from tensorflow.python.keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  __call__(self, *args, **kwargs)\n",
      " |      Wraps `call`, applying pre- and post-processing steps.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        *args: Positional arguments to be passed to `self.call`.\n",
      " |        **kwargs: Keyword arguments to be passed to `self.call`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor(s).\n",
      " |      \n",
      " |      Note:\n",
      " |        - The following optional keyword arguments are reserved for specific uses:\n",
      " |          * `training`: Boolean scalar tensor of Python boolean indicating\n",
      " |            whether the `call` is meant for training or inference.\n",
      " |          * `mask`: Boolean input mask.\n",
      " |        - If the layer's `call` method takes a `mask` argument (as some Keras\n",
      " |          layers do), its default value will be set to the mask generated\n",
      " |          for `inputs` by the previous layer (if `input` did come from\n",
      " |          a layer that generated a corresponding mask, i.e. if it came from\n",
      " |          a Keras layer with masking support.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: if the layer's `call` method returns None (an invalid value).\n",
      " |        RuntimeError: if `super().__init__()` was not called in the constructor.\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Support self.foo = trackable syntax.\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  add_loss(self, losses, inputs=None)\n",
      " |      Add loss tensor(s), potentially dependent on layer inputs.\n",
      " |      \n",
      " |      Some losses (for instance, activity regularization losses) may be dependent\n",
      " |      on the inputs passed when calling a layer. Hence, when reusing the same\n",
      " |      layer on different inputs `a` and `b`, some entries in `layer.losses` may\n",
      " |      be dependent on `a` and some on `b`. This method automatically keeps track\n",
      " |      of dependencies.\n",
      " |      \n",
      " |      This method can be used inside a subclassed layer or model's `call`\n",
      " |      function, in which case `losses` should be a Tensor or list of Tensors.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      class MyLayer(tf.keras.layers.Layer):\n",
      " |        def call(inputs, self):\n",
      " |          self.add_loss(tf.abs(tf.reduce_mean(inputs)), inputs=True)\n",
      " |          return inputs\n",
      " |      ```\n",
      " |      \n",
      " |      This method can also be called directly on a Functional Model during\n",
      " |      construction. In this case, any loss Tensors passed to this Model must\n",
      " |      be symbolic and be able to be traced back to the model's `Input`s. These\n",
      " |      losses become part of the model's topology and are tracked in `get_config`.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      # Activity regularization.\n",
      " |      model.add_loss(tf.abs(tf.reduce_mean(x)))\n",
      " |      ```\n",
      " |      \n",
      " |      If this is not the case for your loss (if, for example, your loss references\n",
      " |      a `Variable` of one of the model's layers), you can wrap your loss in a\n",
      " |      zero-argument lambda. These losses are not tracked as part of the model's\n",
      " |      topology since they can't be serialized.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      # Weight regularization.\n",
      " |      model.add_loss(lambda: tf.reduce_mean(x.kernel))\n",
      " |      ```\n",
      " |      \n",
      " |      The `get_losses_for` method allows to retrieve the losses relevant to a\n",
      " |      specific set of inputs.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses\n",
      " |          may also be zero-argument callables which create a loss tensor.\n",
      " |        inputs: Ignored when executing eagerly. If anything other than None is\n",
      " |          passed, it signals the losses are conditional on some of the layer's\n",
      " |          inputs, and thus they should only be run where these inputs are\n",
      " |          available. This is the case for activity regularization losses, for\n",
      " |          instance. If `None` is passed, the losses are assumed\n",
      " |          to be unconditional, and will apply across all dataflows of the layer\n",
      " |          (e.g. weight regularization losses).\n",
      " |  \n",
      " |  add_metric(self, value, aggregation=None, name=None)\n",
      " |      Adds metric tensor to the layer.\n",
      " |      \n",
      " |      Args:\n",
      " |        value: Metric tensor.\n",
      " |        aggregation: Sample-wise metric reduction function. If `aggregation=None`,\n",
      " |          it indicates that the metric tensor provided has been aggregated\n",
      " |          already. eg, `bin_acc = BinaryAccuracy(name='acc')` followed by\n",
      " |          `model.add_metric(bin_acc(y_true, y_pred))`. If aggregation='mean', the\n",
      " |          given metric tensor will be sample-wise reduced using `mean` function.\n",
      " |          eg, `model.add_metric(tf.reduce_sum(outputs), name='output_mean',\n",
      " |          aggregation='mean')`.\n",
      " |        name: String metric name.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If `aggregation` is anything other than None or `mean`.\n",
      " |  \n",
      " |  add_update(self, updates, inputs=None)\n",
      " |      Add update op(s), potentially dependent on layer inputs. (deprecated arguments)\n",
      " |      \n",
      " |      Warning: SOME ARGUMENTS ARE DEPRECATED: `(inputs)`. They will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      `inputs` is now automatically inferred\n",
      " |      \n",
      " |      Weight updates (for instance, the updates of the moving mean and variance\n",
      " |      in a BatchNormalization layer) may be dependent on the inputs passed\n",
      " |      when calling a layer. Hence, when reusing the same layer on\n",
      " |      different inputs `a` and `b`, some entries in `layer.updates` may be\n",
      " |      dependent on `a` and some on `b`. This method automatically keeps track\n",
      " |      of dependencies.\n",
      " |      \n",
      " |      The `get_updates_for` method allows to retrieve the updates relevant to a\n",
      " |      specific set of inputs.\n",
      " |      \n",
      " |      This call is ignored when eager execution is enabled (in that case, variable\n",
      " |      updates are run on the fly and thus do not need to be tracked for later\n",
      " |      execution).\n",
      " |      \n",
      " |      Arguments:\n",
      " |        updates: Update op, or list/tuple of update ops, or zero-arg callable\n",
      " |          that returns an update op. A zero-arg callable should be passed in\n",
      " |          order to disable running the updates by setting `trainable=False`\n",
      " |          on this Layer, when executing in Eager mode.\n",
      " |        inputs: Deprecated, will be automatically inferred.\n",
      " |  \n",
      " |  add_variable(self, *args, **kwargs)\n",
      " |      Deprecated, do NOT use! Alias for `add_weight`. (deprecated)\n",
      " |      \n",
      " |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      Please use `layer.add_weight` method instead.\n",
      " |  \n",
      " |  add_weight(self, name=None, shape=None, dtype=None, initializer=None, regularizer=None, trainable=None, constraint=None, partitioner=None, use_resource=None, synchronization=<VariableSynchronization.AUTO: 0>, aggregation=<VariableAggregation.NONE: 0>, **kwargs)\n",
      " |      Adds a new variable to the layer.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        name: Variable name.\n",
      " |        shape: Variable shape. Defaults to scalar if unspecified.\n",
      " |        dtype: The type of the variable. Defaults to `self.dtype` or `float32`.\n",
      " |        initializer: Initializer instance (callable).\n",
      " |        regularizer: Regularizer instance (callable).\n",
      " |        trainable: Boolean, whether the variable should be part of the layer's\n",
      " |          \"trainable_variables\" (e.g. variables, biases)\n",
      " |          or \"non_trainable_variables\" (e.g. BatchNorm mean and variance).\n",
      " |          Note that `trainable` cannot be `True` if `synchronization`\n",
      " |          is set to `ON_READ`.\n",
      " |        constraint: Constraint instance (callable).\n",
      " |        partitioner: Partitioner to be passed to the `Trackable` API.\n",
      " |        use_resource: Whether to use `ResourceVariable`.\n",
      " |        synchronization: Indicates when a distributed a variable will be\n",
      " |          aggregated. Accepted values are constants defined in the class\n",
      " |          `tf.VariableSynchronization`. By default the synchronization is set to\n",
      " |          `AUTO` and the current `DistributionStrategy` chooses\n",
      " |          when to synchronize. If `synchronization` is set to `ON_READ`,\n",
      " |          `trainable` must not be set to `True`.\n",
      " |        aggregation: Indicates how a distributed variable will be aggregated.\n",
      " |          Accepted values are constants defined in the class\n",
      " |          `tf.VariableAggregation`.\n",
      " |        **kwargs: Additional keyword arguments. Accepted values are `getter`,\n",
      " |          `collections`, `experimental_autocast` and `caching_device`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The created variable. Usually either a `Variable` or `ResourceVariable`\n",
      " |        instance. If `partitioner` is not `None`, a `PartitionedVariable`\n",
      " |        instance is returned.\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called with partitioned variable regularization and\n",
      " |          eager execution is enabled.\n",
      " |        ValueError: When giving unsupported dtype and no initializer or when\n",
      " |          trainable has been set to True with synchronization set as `ON_READ`.\n",
      " |  \n",
      " |  apply(self, inputs, *args, **kwargs)\n",
      " |      Deprecated, do NOT use! (deprecated)\n",
      " |      \n",
      " |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      Please use `layer.__call__` method instead.\n",
      " |      \n",
      " |      This is an alias of `self.__call__`.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        inputs: Input tensor(s).\n",
      " |        *args: additional positional arguments to be passed to `self.call`.\n",
      " |        **kwargs: additional keyword arguments to be passed to `self.call`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor(s).\n",
      " |  \n",
      " |  compute_mask(self, inputs, mask=None)\n",
      " |      Computes an output mask tensor.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          inputs: Tensor or list of tensors.\n",
      " |          mask: Tensor or list of tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |          None or a tensor (or list of tensors,\n",
      " |              one per output tensor of the layer).\n",
      " |  \n",
      " |  compute_output_signature(self, input_signature)\n",
      " |      Compute the output tensor signature of the layer based on the inputs.\n",
      " |      \n",
      " |      Unlike a TensorShape object, a TensorSpec object contains both shape\n",
      " |      and dtype information for a tensor. This method allows layers to provide\n",
      " |      output dtype information if it is different from the input dtype.\n",
      " |      For any layer that doesn't implement this function,\n",
      " |      the framework will fall back to use `compute_output_shape`, and will\n",
      " |      assume that the output dtype matches the input dtype.\n",
      " |      \n",
      " |      Args:\n",
      " |        input_signature: Single TensorSpec or nested structure of TensorSpec\n",
      " |          objects, describing a candidate input for the layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Single TensorSpec or nested structure of TensorSpec objects, describing\n",
      " |          how the layer would transform the provided input.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If input_signature contains a non-TensorSpec object.\n",
      " |  \n",
      " |  count_params(self)\n",
      " |      Count the total number of scalars composing the weights.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An integer count.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: if the layer isn't yet built\n",
      " |            (in which case its weights aren't yet defined).\n",
      " |  \n",
      " |  get_input_at(self, node_index)\n",
      " |      Retrieves the input tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor (or list of tensors if the layer has multiple inputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_input_mask_at(self, node_index)\n",
      " |      Retrieves the input mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple inputs).\n",
      " |  \n",
      " |  get_input_shape_at(self, node_index)\n",
      " |      Retrieves the input shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple inputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_losses_for(self, inputs)\n",
      " |      Retrieves losses relevant to a specific set of inputs.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        inputs: Input tensor or list/tuple of input tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |        List of loss tensors of the layer that depend on `inputs`.\n",
      " |  \n",
      " |  get_output_at(self, node_index)\n",
      " |      Retrieves the output tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor (or list of tensors if the layer has multiple outputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_output_mask_at(self, node_index)\n",
      " |      Retrieves the output mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple outputs).\n",
      " |  \n",
      " |  get_output_shape_at(self, node_index)\n",
      " |      Retrieves the output shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple outputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_updates_for(self, inputs)\n",
      " |      Retrieves updates relevant to a specific set of inputs.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        inputs: Input tensor or list/tuple of input tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |        List of update ops of the layer that depend on `inputs`.\n",
      " |  \n",
      " |  get_weights(self)\n",
      " |      Returns the current weights of the layer.\n",
      " |      \n",
      " |      The weights of a layer represent the state of the layer. This function\n",
      " |      returns both trainable and non-trainable weight values associated with this\n",
      " |      layer as a list of Numpy arrays, which can in turn be used to load state\n",
      " |      into similarly parameterized layers.\n",
      " |      \n",
      " |      For example, a Dense layer returns a list of two values-- per-output\n",
      " |      weights and the bias value. These can be used to set the weights of another\n",
      " |      Dense layer:\n",
      " |      \n",
      " |      >>> a = tf.keras.layers.Dense(1,\n",
      " |      ...   kernel_initializer=tf.constant_initializer(1.))\n",
      " |      >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]]))\n",
      " |      >>> a.get_weights()\n",
      " |      [array([[1.],\n",
      " |             [1.],\n",
      " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      >>> b = tf.keras.layers.Dense(1,\n",
      " |      ...   kernel_initializer=tf.constant_initializer(2.))\n",
      " |      >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]]))\n",
      " |      >>> b.get_weights()\n",
      " |      [array([[2.],\n",
      " |             [2.],\n",
      " |             [2.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      >>> b.set_weights(a.get_weights())\n",
      " |      >>> b.get_weights()\n",
      " |      [array([[1.],\n",
      " |             [1.],\n",
      " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      \n",
      " |      Returns:\n",
      " |          Weights values as a list of numpy arrays.\n",
      " |  \n",
      " |  set_weights(self, weights)\n",
      " |      Sets the weights of the layer, from Numpy arrays.\n",
      " |      \n",
      " |      The weights of a layer represent the state of the layer. This function\n",
      " |      sets the weight values from numpy arrays. The weight values should be\n",
      " |      passed in the order they are created by the layer. Note that the layer's\n",
      " |      weights must be instantiated before calling this function by calling\n",
      " |      the layer.\n",
      " |      \n",
      " |      For example, a Dense layer returns a list of two values-- per-output\n",
      " |      weights and the bias value. These can be used to set the weights of another\n",
      " |      Dense layer:\n",
      " |      \n",
      " |      >>> a = tf.keras.layers.Dense(1,\n",
      " |      ...   kernel_initializer=tf.constant_initializer(1.))\n",
      " |      >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]]))\n",
      " |      >>> a.get_weights()\n",
      " |      [array([[1.],\n",
      " |             [1.],\n",
      " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      >>> b = tf.keras.layers.Dense(1,\n",
      " |      ...   kernel_initializer=tf.constant_initializer(2.))\n",
      " |      >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]]))\n",
      " |      >>> b.get_weights()\n",
      " |      [array([[2.],\n",
      " |             [2.],\n",
      " |             [2.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      >>> b.set_weights(a.get_weights())\n",
      " |      >>> b.get_weights()\n",
      " |      [array([[1.],\n",
      " |             [1.],\n",
      " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      \n",
      " |      Arguments:\n",
      " |          weights: a list of Numpy arrays. The number\n",
      " |              of arrays and their shape must match\n",
      " |              number of the dimensions of the weights\n",
      " |              of the layer (i.e. it should match the\n",
      " |              output of `get_weights`).\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: If the provided weights list does not match the\n",
      " |              layer's specifications.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from tensorflow.python.keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  from_config(config) from builtins.type\n",
      " |      Creates a layer from its config.\n",
      " |      \n",
      " |      This method is the reverse of `get_config`,\n",
      " |      capable of instantiating the same layer from the config\n",
      " |      dictionary. It does not handle layer connectivity\n",
      " |      (handled by Network), nor weights (handled by `set_weights`).\n",
      " |      \n",
      " |      Arguments:\n",
      " |          config: A Python dictionary, typically the\n",
      " |              output of get_config.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A layer instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  activity_regularizer\n",
      " |      Optional regularizer function for the output of this layer.\n",
      " |  \n",
      " |  dtype\n",
      " |      Dtype used by the weights of the layer, set in the constructor.\n",
      " |  \n",
      " |  dynamic\n",
      " |      Whether the layer is dynamic (eager-only); set in the constructor.\n",
      " |  \n",
      " |  inbound_nodes\n",
      " |      Deprecated, do NOT use! Only for compatibility with external Keras.\n",
      " |  \n",
      " |  input\n",
      " |      Retrieves the input tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one input,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input tensor or list of input tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |        AttributeError: If no inbound nodes are found.\n",
      " |  \n",
      " |  input_mask\n",
      " |      Retrieves the input mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input mask tensor (potentially None) or list of input\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  input_shape\n",
      " |      Retrieves the input shape(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one input,\n",
      " |      i.e. if it is connected to one incoming layer, or if all inputs\n",
      " |      have the same shape.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input shape, as an integer shape tuple\n",
      " |          (or list of shape tuples, one tuple per input tensor).\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer has no defined input_shape.\n",
      " |          RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  input_spec\n",
      " |      `InputSpec` instance(s) describing the input format for this layer.\n",
      " |      \n",
      " |      When you create a layer subclass, you can set `self.input_spec` to enable\n",
      " |      the layer to run input compatibility checks when it is called.\n",
      " |      Consider a `Conv2D` layer: it can only be called on a single input tensor\n",
      " |      of rank 4. As such, you can set, in `__init__()`:\n",
      " |      \n",
      " |      ```python\n",
      " |      self.input_spec = tf.keras.layers.InputSpec(ndim=4)\n",
      " |      ```\n",
      " |      \n",
      " |      Now, if you try to call the layer on an input that isn't rank 4\n",
      " |      (for instance, an input of shape `(2,)`, it will raise a nicely-formatted\n",
      " |      error:\n",
      " |      \n",
      " |      ```\n",
      " |      ValueError: Input 0 of layer conv2d is incompatible with the layer:\n",
      " |      expected ndim=4, found ndim=1. Full shape received: [2]\n",
      " |      ```\n",
      " |      \n",
      " |      Input checks that can be specified via `input_spec` include:\n",
      " |      - Structure (e.g. a single input, a list of 2 inputs, etc)\n",
      " |      - Shape\n",
      " |      - Rank (ndim)\n",
      " |      - Dtype\n",
      " |      \n",
      " |      For more information, see `tf.keras.layers.InputSpec`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `tf.keras.layers.InputSpec` instance, or nested structure thereof.\n",
      " |  \n",
      " |  losses\n",
      " |      Losses which are associated with this `Layer`.\n",
      " |      \n",
      " |      Variable regularization tensors are created when this property is accessed,\n",
      " |      so it is eager safe: accessing `losses` under a `tf.GradientTape` will\n",
      " |      propagate gradients back to the corresponding variables.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of tensors.\n",
      " |  \n",
      " |  metrics\n",
      " |      List of `tf.keras.metrics.Metric` instances tracked by the layer.\n",
      " |  \n",
      " |  name\n",
      " |      Name of the layer (string), set in the constructor.\n",
      " |  \n",
      " |  non_trainable_variables\n",
      " |  \n",
      " |  non_trainable_weights\n",
      " |      List of all non-trainable weights tracked by this layer.\n",
      " |      \n",
      " |      Non-trainable weights are *not* updated during training. They are expected\n",
      " |      to be updated manually in `call()`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of non-trainable variables.\n",
      " |  \n",
      " |  outbound_nodes\n",
      " |      Deprecated, do NOT use! Only for compatibility with external Keras.\n",
      " |  \n",
      " |  output\n",
      " |      Retrieves the output tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one output,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor or list of output tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |        AttributeError: if the layer is connected to more than one incoming\n",
      " |          layers.\n",
      " |        RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  output_mask\n",
      " |      Retrieves the output mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Output mask tensor (potentially None) or list of output\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  output_shape\n",
      " |      Retrieves the output shape(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has one output,\n",
      " |      or if all outputs have the same shape.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Output shape, as an integer shape tuple\n",
      " |          (or list of shape tuples, one tuple per output tensor).\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer has no defined output shape.\n",
      " |          RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  stateful\n",
      " |  \n",
      " |  trainable\n",
      " |  \n",
      " |  trainable_variables\n",
      " |      Sequence of trainable variables owned by this module and its submodules.\n",
      " |      \n",
      " |      Note: this method uses reflection to find variables on the current instance\n",
      " |      and submodules. For performance reasons you may wish to cache the result\n",
      " |      of calling this method if you don't expect the return value to change.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A sequence of variables for the current module (sorted by attribute\n",
      " |        name) followed by variables from all submodules recursively (breadth\n",
      " |        first).\n",
      " |  \n",
      " |  trainable_weights\n",
      " |      List of all trainable weights tracked by this layer.\n",
      " |      \n",
      " |      Trainable weights are updated via gradient descent during training.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of trainable variables.\n",
      " |  \n",
      " |  updates\n",
      " |  \n",
      " |  variables\n",
      " |      Returns the list of all layer variables/weights.\n",
      " |      \n",
      " |      Alias of `self.weights`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of variables.\n",
      " |  \n",
      " |  weights\n",
      " |      Returns the list of all layer variables/weights.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of variables.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from tensorflow.python.module.module.Module:\n",
      " |  \n",
      " |  with_name_scope(method) from builtins.type\n",
      " |      Decorator to automatically enter the module name scope.\n",
      " |      \n",
      " |      >>> class MyModule(tf.Module):\n",
      " |      ...   @tf.Module.with_name_scope\n",
      " |      ...   def __call__(self, x):\n",
      " |      ...     if not hasattr(self, 'w'):\n",
      " |      ...       self.w = tf.Variable(tf.random.normal([x.shape[1], 3]))\n",
      " |      ...     return tf.matmul(x, self.w)\n",
      " |      \n",
      " |      Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose\n",
      " |      names included the module name:\n",
      " |      \n",
      " |      >>> mod = MyModule()\n",
      " |      >>> mod(tf.ones([1, 2]))\n",
      " |      <tf.Tensor: shape=(1, 3), dtype=float32, numpy=..., dtype=float32)>\n",
      " |      >>> mod.w\n",
      " |      <tf.Variable 'my_module/Variable:0' shape=(2, 3) dtype=float32,\n",
      " |      numpy=..., dtype=float32)>\n",
      " |      \n",
      " |      Args:\n",
      " |        method: The method to wrap.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The original method wrapped such that it enters the module's name scope.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.module.module.Module:\n",
      " |  \n",
      " |  name_scope\n",
      " |      Returns a `tf.name_scope` instance for this class.\n",
      " |  \n",
      " |  submodules\n",
      " |      Sequence of all sub-modules.\n",
      " |      \n",
      " |      Submodules are modules which are properties of this module, or found as\n",
      " |      properties of modules which are properties of this module (and so on).\n",
      " |      \n",
      " |      >>> a = tf.Module()\n",
      " |      >>> b = tf.Module()\n",
      " |      >>> c = tf.Module()\n",
      " |      >>> a.b = b\n",
      " |      >>> b.c = c\n",
      " |      >>> list(a.submodules) == [b, c]\n",
      " |      True\n",
      " |      >>> list(b.submodules) == [c]\n",
      " |      True\n",
      " |      >>> list(c.submodules) == []\n",
      " |      True\n",
      " |      \n",
      " |      Returns:\n",
      " |        A sequence of all submodules.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.training.tracking.base.Trackable:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from tensorflow.python.keras.utils.version_utils.LayerVersionSelector:\n",
      " |  \n",
      " |  __new__(cls, *args, **kwargs)\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _california_housing_dataset:\n",
      "\n",
      "California Housing dataset\n",
      "--------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 20640\n",
      "\n",
      "    :Number of Attributes: 8 numeric, predictive attributes and the target\n",
      "\n",
      "    :Attribute Information:\n",
      "        - MedInc        median income in block\n",
      "        - HouseAge      median house age in block\n",
      "        - AveRooms      average number of rooms\n",
      "        - AveBedrms     average number of bedrooms\n",
      "        - Population    block population\n",
      "        - AveOccup      average house occupancy\n",
      "        - Latitude      house block latitude\n",
      "        - Longitude     house block longitude\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "This dataset was obtained from the StatLib repository.\n",
      "http://lib.stat.cmu.edu/datasets/\n",
      "\n",
      "The target variable is the median house value for California districts.\n",
      "\n",
      "This dataset was derived from the 1990 U.S. census, using one row per census\n",
      "block group. A block group is the smallest geographical unit for which the U.S.\n",
      "Census Bureau publishes sample data (a block group typically has a population\n",
      "of 600 to 3,000 people).\n",
      "\n",
      "It can be downloaded/loaded using the\n",
      ":func:`sklearn.datasets.fetch_california_housing` function.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "    - Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\n",
      "      Statistics and Probability Letters, 33 (1997) 291-297\n",
      "\n",
      "(20640, 8)\n",
      "(20640,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "print(housing.DESCR)\n",
    "print(housing.data.shape)\n",
    "print(housing.target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11610, 8) (11610,)\n",
      "(3870, 8) (3870,)\n",
      "(5160, 8) (5160,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train_all, x_test, y_train_all, y_test = train_test_split(\n",
    "    housing.data, housing.target, random_state = 7)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(\n",
    "    x_train_all, y_train_all, random_state = 11)\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_valid.shape, y_valid.shape)\n",
    "print(x_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_valid_scaled = scaler.transform(x_valid)\n",
    "x_test_scaled = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([4.5417706e-05 6.7153489e-03 6.9314718e-01 5.0067153e+00 1.0000046e+01], shape=(5,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#使用lamba，一行代码就搞定\n",
    "# tf.nn.softplus : log(1+e^x)\n",
    "customized_softplus = keras.layers.Lambda(lambda x : tf.nn.softplus(x))\n",
    "print(customized_softplus([-10., -5., 0., 5., 10.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.006715348489117967"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(1+np.exp(-5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "customized_dense_layer_2 (Cu (None, 30)                270       \n",
      "_________________________________________________________________\n",
      "customized_dense_layer_3 (Cu (None, 1)                 31        \n",
      "_________________________________________________________________\n",
      "lambda (Lambda)              (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 301\n",
      "Trainable params: 301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# customized dense layer.\n",
    "class CustomizedDenseLayer(keras.layers.Layer):\n",
    "    def __init__(self, units, activation=None, **kwargs):\n",
    "        self.units = units\n",
    "        self.activation = keras.layers.Activation(activation)\n",
    "        super(CustomizedDenseLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        \"\"\"构建所需要的参数，也就是kernel还有bias\"\"\"\n",
    "        # x * w + b. input_shape:[None, a] w:[a,b]output_shape: [None, b]\n",
    "        self.kernel = self.add_weight(name = 'kernel',\n",
    "                                      shape = (input_shape[1], self.units),\n",
    "                                      initializer = 'uniform',#使用均匀分布的方法去初始化kernel\n",
    "                                      trainable = True)\n",
    "        self.bias = self.add_weight(name = 'bias',\n",
    "                                    shape = (self.units, ),\n",
    "                                    initializer = 'zeros',\n",
    "                                    trainable = True)\n",
    "        #接着我们要继承父类的build\n",
    "        super(CustomizedDenseLayer, self).build(input_shape)\n",
    "    \n",
    "    def call(self, x):\n",
    "        \"\"\"完成正向计算\"\"\"\n",
    "        return self.activation(x @ self.kernel + self.bias)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    CustomizedDenseLayer(30, activation='relu',\n",
    "                         input_shape=x_train.shape[1:]),\n",
    "    CustomizedDenseLayer(1),\n",
    "    #再加一个激活函数层，这个和下面注释的两行等价的\n",
    "    customized_softplus,\n",
    "    # keras.layers.Dense(1, activation=\"softplus\"),  #一层\n",
    "    # keras.layers.Dense(1), keras.layers.Activation('softplus'), #两层\n",
    "])\n",
    "model.summary()\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=\"sgd\")\n",
    "callbacks = [keras.callbacks.EarlyStopping(\n",
    "    patience=5, min_delta=1e-3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 1.2361 - val_loss: 0.6806\n",
      "Epoch 2/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5919 - val_loss: 0.5883\n",
      "Epoch 3/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5152 - val_loss: 0.5225\n",
      "Epoch 4/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4712 - val_loss: 0.4828\n",
      "Epoch 5/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4616 - val_loss: 0.5213\n",
      "Epoch 6/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4563 - val_loss: 0.4581\n",
      "Epoch 7/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4357 - val_loss: 0.4563\n",
      "Epoch 8/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4281 - val_loss: 0.4419\n",
      "Epoch 9/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4209 - val_loss: 0.4400\n",
      "Epoch 10/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4089 - val_loss: 0.4271\n",
      "Epoch 11/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4035 - val_loss: 0.4452\n",
      "Epoch 12/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3998 - val_loss: 0.4130\n",
      "Epoch 13/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3931 - val_loss: 0.4182\n",
      "Epoch 14/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3891 - val_loss: 0.4022\n",
      "Epoch 15/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3861 - val_loss: 0.4094\n",
      "Epoch 16/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3819 - val_loss: 0.3965\n",
      "Epoch 17/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3788 - val_loss: 0.4054\n",
      "Epoch 18/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3773 - val_loss: 0.3934\n",
      "Epoch 19/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3761 - val_loss: 0.3963\n",
      "Epoch 20/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3740 - val_loss: 0.3885\n",
      "Epoch 21/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3730 - val_loss: 0.3871\n",
      "Epoch 22/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3721 - val_loss: 0.3873\n",
      "Epoch 23/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3703 - val_loss: 0.3890\n",
      "Epoch 24/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3697 - val_loss: 0.3868\n",
      "Epoch 25/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3683 - val_loss: 0.3851\n",
      "Epoch 26/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3670 - val_loss: 0.3810\n",
      "Epoch 27/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3664 - val_loss: 0.3850\n",
      "Epoch 28/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3651 - val_loss: 0.3860\n",
      "Epoch 29/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3637 - val_loss: 0.3804\n",
      "Epoch 30/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3641 - val_loss: 0.3807\n",
      "Epoch 31/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3628 - val_loss: 0.3854\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train_scaled, y_train,\n",
    "                    validation_data = (x_valid_scaled, y_valid),\n",
    "                    epochs = 100,\n",
    "                    callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_learning_curves(history):\n",
    "    pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "    plt.grid(True)\n",
    "    plt.gca().set_ylim(0, 1)\n",
    "    plt.show()\n",
    "plot_learning_curves(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38484829664230347"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test_scaled, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
